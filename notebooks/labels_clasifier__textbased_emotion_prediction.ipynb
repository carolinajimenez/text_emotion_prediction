{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning Challenge\n",
        "## Sentiment Analysis for Text => Toxic Comment Classification"
      ],
      "metadata": {
        "id": "765WO1hcoboN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "__author__ = \"Carolina Jiménez Moreno <cjimenezm0794@gmail.com>\"\n",
        "__version__ = \"1.0.0\""
      ],
      "metadata": {
        "id": "H3_3KtQTod7C"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmsH5z2WTxjh",
        "outputId": "1a2ac601-1ec0-445d-a800-fe52bf363962"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load dataset"
      ],
      "metadata": {
        "id": "RCpNxWQRoffE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "-ia6wC0cy7Zm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mUFhcDCcu6gz"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv('/content/drive/MyDrive/train.csv')\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/test.csv')\n",
        "test_labels = pd.read_csv('/content/drive/MyDrive/test_labels.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = test_df.merge(test_labels,how='inner',on='id')\n",
        "test_df = test_df[test_df['toxic']!=-1]"
      ],
      "metadata": {
        "id": "tMjWUSIhzr6H"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's label datasets as train and test and then divide\n",
        "test_df['type'] = 'test'\n",
        "train_df['type'] = 'train'"
      ],
      "metadata": {
        "id": "EC5XoFnz4Btq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's concatenate datasets to use the words from both files as the universe\n",
        "train_df = pd.concat([train_df,test_df],ignore_index=True)"
      ],
      "metadata": {
        "id": "baycHf444pxH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def clean_text(text):\n",
        "    # Lemmatizing the texts\n",
        "    # removing aphostrophe words\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"what's\", \"what is \",str(text)) \n",
        "    text = re.sub(r\"'s\", \" \", str(text)) \n",
        "    text = re.sub(r\"'ve\", \" have \", str(text)) \n",
        "    text = re.sub(r\"can't\", \"cannot \", str(text)) \n",
        "    text = re.sub(r\"ain't\", 'is not', str(text)) \n",
        "    text = re.sub(r\"won't\", 'will not', str(text)) \n",
        "    text = re.sub(r\"n't\", \" not \", str(text)) \n",
        "    text = re.sub(r\"i'm\", \"i am \", str(text)) \n",
        "    text = re.sub(r\"'re\", \" are \", str(text)) \n",
        "    text = re.sub(r\"'d\", \" would \", str(text)) \n",
        "    text = re.sub(r\"'ll\", \" will \", str(text)) \n",
        "    text = re.sub(r\"'scuse\", \" excuse \", str(text)) \n",
        "    text = re.sub('W', ' ', str(text)) \n",
        "    text = re.sub(' +', ' ', str(text))\n",
        "    # Remove hyperlinks\n",
        "    text = re.sub(r\"https?://\\S+|www\\.\\S+\", ' ', str(text))\n",
        "    # Remove punctuations, numbers and special characters\n",
        "    text = re.sub('[^A-Za-z0-9]+', ' ', str(text))\n",
        "    text = lemmatizer.lemmatize(text)\n",
        "    text = text.strip(' ')\n",
        "    return text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLLwtXfno73W",
        "outputId": "a9db55a0-d0fb-41f8-a6f2-4884824211ad"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's clean up the comment_text in train\n",
        "train_df['comment_text'] = train_df['comment_text'].map(lambda comment : clean_text(comment))"
      ],
      "metadata": {
        "id": "wPmmYnB_pAa4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's create splitted field with lists of the comment words\n",
        "train_df['splitted'] = train_df['comment_text'].apply(lambda x :x.split()  )"
      ],
      "metadata": {
        "id": "I2NJshYe33wP"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's remove single letter words from the list\n",
        "def remove_singles(x):\n",
        "  return [i for i in x if len(i)>1]\n",
        "train_df['splitted'] = train_df['splitted'].apply(lambda x :  remove_singles(x))"
      ],
      "metadata": {
        "id": "PkihXGTC-BUl"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's remove the numbers\n",
        "def remove_numbers(x):\n",
        "  return [i for i in x if not i.isdigit()]\n",
        "train_df['splitted'] = train_df['splitted'].apply(lambda x :  remove_numbers(x))"
      ],
      "metadata": {
        "id": "-8aHIIdB_Gqg"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's remove what has letters and numbers\n",
        "def remove_alphanumbers(my_list):\n",
        "  return [x for x in my_list if not any(c.isdigit() for c in x)]\n",
        "train_df['splitted'] = train_df['splitted'].apply(lambda x: remove_alphanumbers(x))"
      ],
      "metadata": {
        "id": "osLGR_jjMVux"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's create a field with the length of each list of words\n",
        "train_df['len'] = train_df['splitted'].apply(lambda x :len(x))"
      ],
      "metadata": {
        "id": "fl6ZjPy6a_jN"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df"
      ],
      "metadata": {
        "id": "pZ1EjRbG4-kc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "f3eea371-f7f9-44d0-b74c-130ecb1d21e5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                      id                                       comment_text  \\\n",
              "0       0000997932d777bf  explanation why the edits made under my userna...   \n",
              "1       000103f0d9cfb60f  d aww he matches this background colour i am s...   \n",
              "2       000113f07ec002fd  hey man i am really not trying to edit war it ...   \n",
              "3       0001b41b1c6bb37e  more i cannot make any real suggestions on imp...   \n",
              "4       0001d958c54c6e35  you sir are my hero any chance you remember wh...   \n",
              "...                  ...                                                ...   \n",
              "223544  fff8f64043129fa2  jerome i see you never got around to this i m ...   \n",
              "223545  fff9d70fe0722906  lucky bastard heh you are famous now i kida en...   \n",
              "223546  fffa8a11c4378854  shame on you all you want to speak about gays ...   \n",
              "223547  fffac2a094c8e0e2  mel gibson is a nazi bitch who makes shitty mo...   \n",
              "223548  fffb5451268fb5ba  unicorn lair discovery supposedly a unicorn la...   \n",
              "\n",
              "        toxic  severe_toxic  obscene  threat  insult  identity_hate   type  \\\n",
              "0           0             0        0       0       0              0  train   \n",
              "1           0             0        0       0       0              0  train   \n",
              "2           0             0        0       0       0              0  train   \n",
              "3           0             0        0       0       0              0  train   \n",
              "4           0             0        0       0       0              0  train   \n",
              "...       ...           ...      ...     ...     ...            ...    ...   \n",
              "223544      0             0        0       0       0              0   test   \n",
              "223545      0             0        0       0       0              0   test   \n",
              "223546      0             0        0       0       0              0   test   \n",
              "223547      1             0        1       0       1              0   test   \n",
              "223548      0             0        0       0       0              0   test   \n",
              "\n",
              "                                                 splitted  len  \n",
              "0       [explanation, why, the, edits, made, under, my...   44  \n",
              "1       [aww, he, matches, this, background, colour, a...   14  \n",
              "2       [hey, man, am, really, not, trying, to, edit, ...   42  \n",
              "3       [more, cannot, make, any, real, suggestions, o...  105  \n",
              "4       [you, sir, are, my, hero, any, chance, you, re...   13  \n",
              "...                                                   ...  ...  \n",
              "223544  [jerome, see, you, never, got, around, to, thi...  101  \n",
              "223545  [lucky, bastard, heh, you, are, famous, now, k...   11  \n",
              "223546  [shame, on, you, all, you, want, to, speak, ab...   14  \n",
              "223547  [mel, gibson, is, nazi, bitch, who, makes, shi...   25  \n",
              "223548  [unicorn, lair, discovery, supposedly, unicorn...   39  \n",
              "\n",
              "[223549 rows x 11 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1904215d-8808-44c8-8760-4ed7636339df\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "      <th>type</th>\n",
              "      <th>splitted</th>\n",
              "      <th>len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0000997932d777bf</td>\n",
              "      <td>explanation why the edits made under my userna...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "      <td>[explanation, why, the, edits, made, under, my...</td>\n",
              "      <td>44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000103f0d9cfb60f</td>\n",
              "      <td>d aww he matches this background colour i am s...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "      <td>[aww, he, matches, this, background, colour, a...</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>000113f07ec002fd</td>\n",
              "      <td>hey man i am really not trying to edit war it ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "      <td>[hey, man, am, really, not, trying, to, edit, ...</td>\n",
              "      <td>42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0001b41b1c6bb37e</td>\n",
              "      <td>more i cannot make any real suggestions on imp...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "      <td>[more, cannot, make, any, real, suggestions, o...</td>\n",
              "      <td>105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0001d958c54c6e35</td>\n",
              "      <td>you sir are my hero any chance you remember wh...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "      <td>[you, sir, are, my, hero, any, chance, you, re...</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>223544</th>\n",
              "      <td>fff8f64043129fa2</td>\n",
              "      <td>jerome i see you never got around to this i m ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>test</td>\n",
              "      <td>[jerome, see, you, never, got, around, to, thi...</td>\n",
              "      <td>101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>223545</th>\n",
              "      <td>fff9d70fe0722906</td>\n",
              "      <td>lucky bastard heh you are famous now i kida en...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>test</td>\n",
              "      <td>[lucky, bastard, heh, you, are, famous, now, k...</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>223546</th>\n",
              "      <td>fffa8a11c4378854</td>\n",
              "      <td>shame on you all you want to speak about gays ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>test</td>\n",
              "      <td>[shame, on, you, all, you, want, to, speak, ab...</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>223547</th>\n",
              "      <td>fffac2a094c8e0e2</td>\n",
              "      <td>mel gibson is a nazi bitch who makes shitty mo...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>test</td>\n",
              "      <td>[mel, gibson, is, nazi, bitch, who, makes, shi...</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>223548</th>\n",
              "      <td>fffb5451268fb5ba</td>\n",
              "      <td>unicorn lair discovery supposedly a unicorn la...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>test</td>\n",
              "      <td>[unicorn, lair, discovery, supposedly, unicorn...</td>\n",
              "      <td>39</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>223549 rows × 11 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1904215d-8808-44c8-8760-4ed7636339df')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1904215d-8808-44c8-8760-4ed7636339df button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1904215d-8808-44c8-8760-4ed7636339df');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create dictionary"
      ],
      "metadata": {
        "id": "rX1iHrJXp2ga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader,TensorDataset,Dataset\n",
        "import torchvision\n",
        "import torch\n",
        "from IPython.display import clear_output"
      ],
      "metadata": {
        "id": "zVUx80ko3iZu"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter comments that have garbage\n",
        "train_df = train_df[train_df['len']>0]"
      ],
      "metadata": {
        "id": "YWoe0yn9fP8M"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's create a list with all the words in the comments\n",
        "word_list = []\n",
        "for i in train_df.index:\n",
        "  for w in train_df.loc[i,'splitted']:\n",
        "    word_list.append(w)"
      ],
      "metadata": {
        "id": "fLk6RJgn37ZG"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a df with universe of words\n",
        "words_df = pd.DataFrame({'w':word_list})"
      ],
      "metadata": {
        "id": "T8ONPBgR7y3-"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove repeated words\n",
        "word_corpus = list(words_df['w'].unique())"
      ],
      "metadata": {
        "id": "o2Kd4cFY8KBr"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort the list\n",
        "word_corpus.sort()"
      ],
      "metadata": {
        "id": "dgaSLb5SNAAi"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary of the form 'word': 'number'\n",
        "word_dict = {word_corpus[i]:i for i in range(len(word_corpus))}"
      ],
      "metadata": {
        "id": "QN2yIQM-8ZXQ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_dict"
      ],
      "metadata": {
        "id": "sfDKtX2Sb2hJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f43ccd6-91fd-41b9-c14c-81bd120d883b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'aa': 0,\n",
              " 'aaa': 1,\n",
              " 'aaaa': 2,\n",
              " 'aaaaa': 3,\n",
              " 'aaaaaaaa': 4,\n",
              " 'aaaaaaaaaaaaaaaaaaaaaaaaa': 5,\n",
              " 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaalllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll': 6,\n",
              " 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaahhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh': 7,\n",
              " 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaany': 8,\n",
              " 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaahhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh': 9,\n",
              " 'aaaaaaaaaaaaaaaaaaaaaaaaaaahhhhhhhhhhhhhhhhhhhhhhhhhhhhh': 10,\n",
              " 'aaaaaaaaaaaaaaaaaaaalllllllllllllllllllllll': 11,\n",
              " 'aaaaaaaaaaaaaaaaaaggggggggggggggggggggggggggggggggggggggggggggggg': 12,\n",
              " 'aaaaaaaaaaaahahahahahahaaaaaaaaaaaaaahahahahahaaaaaaaaaaaaaaahahahahaaaaaaaaaaaaaaaaaaaaaaa': 13,\n",
              " 'aaaaaaaaaaarrrrrrrrrggggggg': 14,\n",
              " 'aaaaaaaaaah': 15,\n",
              " 'aaaaaaaaaahhhhhhhhhhhhhh': 16,\n",
              " 'aaaaaaaaaannnnnnnnnnnnnnnnaaaaaaaaaaaaaaaaaaalllllllllll': 17,\n",
              " 'aaaaaaaahhhhhhhhhhhhhhhhhhhhhhh': 18,\n",
              " 'aaaaaaaahhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh': 19,\n",
              " 'aaaaaaaarrrrrrrrgggggggghhhhhhhhh': 20,\n",
              " 'aaaaaaahhhhhhhhhhhhhhhhhhhhhhhh': 21,\n",
              " 'aaaaaaargh': 22,\n",
              " 'aaaaaarrrrrrgghhh': 23,\n",
              " 'aaaaaaw': 24,\n",
              " 'aaaaah': 25,\n",
              " 'aaaaargh': 26,\n",
              " 'aaaaarrrrrrggggghhhhh': 27,\n",
              " 'aaaah': 28,\n",
              " 'aaaahhhhhhhh': 29,\n",
              " 'aaaall': 30,\n",
              " 'aaaand': 31,\n",
              " 'aaaannnnyyyywwwwhhhheeeerrrreeee': 32,\n",
              " 'aaaarrrggghhhh': 33,\n",
              " 'aaaawwww': 34,\n",
              " 'aaages': 35,\n",
              " 'aaagh': 36,\n",
              " 'aaaghh': 37,\n",
              " 'aaah': 38,\n",
              " 'aaahhh': 39,\n",
              " 'aaahs': 40,\n",
              " 'aaai': 41,\n",
              " 'aaajade': 42,\n",
              " 'aaand': 43,\n",
              " 'aaarrrgggh': 44,\n",
              " 'aaassssss': 45,\n",
              " 'aaaww': 46,\n",
              " 'aaayayayayayayaaaaaa': 47,\n",
              " 'aab': 48,\n",
              " 'aaba': 49,\n",
              " 'aabacaana': 50,\n",
              " 'aaberg': 51,\n",
              " 'aabove': 52,\n",
              " 'aac': 53,\n",
              " 'aacd': 54,\n",
              " 'aachen': 55,\n",
              " 'aachi': 56,\n",
              " 'aacs': 57,\n",
              " 'aacsb': 58,\n",
              " 'aad': 59,\n",
              " 'aadd': 60,\n",
              " 'aademia': 61,\n",
              " 'aadhaar': 62,\n",
              " 'aadil': 63,\n",
              " 'aadmi': 64,\n",
              " 'aadu': 65,\n",
              " 'aaf': 66,\n",
              " 'aafakhravar': 67,\n",
              " 'aafco': 68,\n",
              " 'aaffect': 69,\n",
              " 'aafia': 70,\n",
              " 'aafs': 71,\n",
              " 'aag': 72,\n",
              " 'aagadu': 73,\n",
              " 'aage': 74,\n",
              " 'aagf': 75,\n",
              " 'aagin': 76,\n",
              " 'aagosh': 77,\n",
              " 'aah': 78,\n",
              " 'aahahahahahaha': 79,\n",
              " 'aahahahahahahjaahahahahahahaahh': 80,\n",
              " 'aahank': 81,\n",
              " 'aahh': 82,\n",
              " 'aahhaha': 83,\n",
              " 'aahil': 84,\n",
              " 'aahj': 85,\n",
              " 'aahoa': 86,\n",
              " 'aai': 87,\n",
              " 'aaiha': 88,\n",
              " 'aaj': 89,\n",
              " 'aajacksoniv': 90,\n",
              " 'aajonus': 91,\n",
              " 'aakash': 92,\n",
              " 'aake': 93,\n",
              " 'aal': 94,\n",
              " 'aalborg': 95,\n",
              " 'aalertbot': 96,\n",
              " 'aaliya': 97,\n",
              " 'aaliyah': 98,\n",
              " 'aall': 99,\n",
              " 'aallalalalalalalalallalallalal': 100,\n",
              " 'aals': 101,\n",
              " 'aalst': 102,\n",
              " 'aam': 103,\n",
              " 'aamadmiofworld': 104,\n",
              " 'aamanda': 105,\n",
              " 'aamer': 106,\n",
              " 'aami': 107,\n",
              " 'aamir': 108,\n",
              " 'aaml': 109,\n",
              " 'aamof': 110,\n",
              " 'aams': 111,\n",
              " 'aamu': 112,\n",
              " 'aan': 113,\n",
              " 'aana': 114,\n",
              " 'aanand': 115,\n",
              " 'aanas': 116,\n",
              " 'aancc': 117,\n",
              " 'aand': 118,\n",
              " 'aandahl': 119,\n",
              " 'aanddd': 120,\n",
              " 'aanders': 121,\n",
              " 'aandhi': 122,\n",
              " 'aanestad': 123,\n",
              " 'aang': 124,\n",
              " 'aangebracht': 125,\n",
              " 'aangemaakt': 126,\n",
              " 'aankhiya': 127,\n",
              " 'aanklachten': 128,\n",
              " 'aankomst': 129,\n",
              " 'aantal': 130,\n",
              " 'aantoodu': 131,\n",
              " 'aanwezig': 132,\n",
              " 'aao': 133,\n",
              " 'aaot': 134,\n",
              " 'aap': 135,\n",
              " 'aapg': 136,\n",
              " 'aapl': 137,\n",
              " 'aapn': 138,\n",
              " 'aapne': 139,\n",
              " 'aapropriate': 140,\n",
              " 'aapt': 141,\n",
              " 'aaqib': 142,\n",
              " 'aar': 143,\n",
              " 'aarabs': 144,\n",
              " 'aarau': 145,\n",
              " 'aardman': 146,\n",
              " 'aardsma': 147,\n",
              " 'aardsman': 148,\n",
              " 'aardvark': 149,\n",
              " 'aardvarks': 150,\n",
              " 'aare': 151,\n",
              " 'aarem': 152,\n",
              " 'aargh': 153,\n",
              " 'aarhus': 154,\n",
              " 'aarionrhod': 155,\n",
              " 'aarm': 156,\n",
              " 'aarne': 157,\n",
              " 'aaroamal': 158,\n",
              " 'aarohi': 159,\n",
              " 'aaron': 160,\n",
              " 'aaroncrick': 161,\n",
              " 'aaronic': 162,\n",
              " 'aaronovitch': 163,\n",
              " 'aarons': 164,\n",
              " 'aaronshavit': 165,\n",
              " 'aaronsw': 166,\n",
              " 'aaround': 167,\n",
              " 'aarp': 168,\n",
              " 'aarresaaren': 169,\n",
              " 'aarrow': 170,\n",
              " 'aarrrrrrgh': 171,\n",
              " 'aaruveetil': 172,\n",
              " 'aas': 173,\n",
              " 'aascar': 174,\n",
              " 'aashiq': 175,\n",
              " 'aashis': 176,\n",
              " 'aashish': 177,\n",
              " 'aashishpaatidaar': 178,\n",
              " 'aashto': 179,\n",
              " 'aassssaartfufsiiinnnnssss': 180,\n",
              " 'aat': 181,\n",
              " 'aatc': 182,\n",
              " 'aatg': 183,\n",
              " 'aatma': 184,\n",
              " 'aau': 185,\n",
              " 'aaup': 186,\n",
              " 'aav': 187,\n",
              " 'aave': 188,\n",
              " 'aaviksoo': 189,\n",
              " 'aavishkaar': 190,\n",
              " 'aaw': 191,\n",
              " 'aawwweeessssoooommmmeeeee': 192,\n",
              " 'aayege': 193,\n",
              " 'aayelai': 194,\n",
              " 'aayeli': 195,\n",
              " 'ab': 196,\n",
              " 'aba': 197,\n",
              " 'abab': 198,\n",
              " 'ababy': 199,\n",
              " 'abaca': 200,\n",
              " 'abacha': 201,\n",
              " 'abacination': 202,\n",
              " 'aback': 203,\n",
              " 'abaco': 204,\n",
              " 'abacus': 205,\n",
              " 'abad': 206,\n",
              " 'abadan': 207,\n",
              " 'abaddon': 208,\n",
              " 'abade': 209,\n",
              " 'abadesa': 210,\n",
              " 'abagnale': 211,\n",
              " 'abaixo': 212,\n",
              " 'abakumov': 213,\n",
              " 'abal': 214,\n",
              " 'abalessa': 215,\n",
              " 'abali': 216,\n",
              " 'aban': 217,\n",
              " 'abanagi': 218,\n",
              " 'abandon': 219,\n",
              " 'abandonded': 220,\n",
              " 'abandoned': 221,\n",
              " 'abandoning': 222,\n",
              " 'abandonment': 223,\n",
              " 'abandonments': 224,\n",
              " 'abandonned': 225,\n",
              " 'abandonou': 226,\n",
              " 'abandons': 227,\n",
              " 'abanes': 228,\n",
              " 'abang': 229,\n",
              " 'abantecart': 230,\n",
              " 'abany': 231,\n",
              " 'abaondond': 232,\n",
              " 'abaranger': 233,\n",
              " 'abarat': 234,\n",
              " 'abarbanel': 235,\n",
              " 'abarenoh': 236,\n",
              " 'abarth': 237,\n",
              " 'abartig': 238,\n",
              " 'abase': 239,\n",
              " 'abased': 240,\n",
              " 'abassids': 241,\n",
              " 'abatayo': 242,\n",
              " 'abate': 243,\n",
              " 'abated': 244,\n",
              " 'abatement': 245,\n",
              " 'abati': 246,\n",
              " 'abay': 247,\n",
              " 'abaya': 248,\n",
              " 'abb': 249,\n",
              " 'abba': 250,\n",
              " 'abbadon': 251,\n",
              " 'abbas': 252,\n",
              " 'abbasaheb': 253,\n",
              " 'abbasgulu': 254,\n",
              " 'abbasi': 255,\n",
              " 'abbasid': 256,\n",
              " 'abbasids': 257,\n",
              " 'abbass': 258,\n",
              " 'abbassed': 259,\n",
              " 'abbassi': 260,\n",
              " 'abbassid': 261,\n",
              " 'abbastanza': 262,\n",
              " 'abbaton': 263,\n",
              " 'abbau': 264,\n",
              " 'abbe': 265,\n",
              " 'abberant': 266,\n",
              " 'abberation': 267,\n",
              " 'abberations': 268,\n",
              " 'abberline': 269,\n",
              " 'abberrant': 270,\n",
              " 'abbes': 271,\n",
              " 'abbess': 272,\n",
              " 'abbetting': 273,\n",
              " 'abbey': 274,\n",
              " 'abbeys': 275,\n",
              " 'abbeyside': 276,\n",
              " 'abbi': 277,\n",
              " 'abbie': 278,\n",
              " 'abbitt': 279,\n",
              " 'abbled': 280,\n",
              " 'abbles': 281,\n",
              " 'abbos': 282,\n",
              " 'abbot': 283,\n",
              " 'abbots': 284,\n",
              " 'abbott': 285,\n",
              " 'abbottabad': 286,\n",
              " 'abbottsford': 287,\n",
              " 'abbout': 288,\n",
              " 'abbr': 289,\n",
              " 'abbrasive': 290,\n",
              " 'abbrev': 291,\n",
              " 'abbrevation': 292,\n",
              " 'abbreveated': 293,\n",
              " 'abbreviate': 294,\n",
              " 'abbreviated': 295,\n",
              " 'abbreviatedis': 296,\n",
              " 'abbreviates': 297,\n",
              " 'abbreviating': 298,\n",
              " 'abbreviation': 299,\n",
              " 'abbreviations': 300,\n",
              " 'abbrevs': 301,\n",
              " 'abbriviations': 302,\n",
              " 'abbs': 303,\n",
              " 'abbusively': 304,\n",
              " 'abby': 305,\n",
              " 'abbyses': 306,\n",
              " 'abbythecat': 307,\n",
              " 'abbywinters': 308,\n",
              " 'abbyy': 309,\n",
              " 'abc': 310,\n",
              " 'abcd': 311,\n",
              " 'abcde': 312,\n",
              " 'abcdef': 313,\n",
              " 'abcedare': 314,\n",
              " 'abcedere': 315,\n",
              " 'abcef': 316,\n",
              " 'abchasen': 317,\n",
              " 'abcmonster': 318,\n",
              " 'abcnews': 319,\n",
              " 'abcom': 320,\n",
              " 'abcs': 321,\n",
              " 'abd': 322,\n",
              " 'abdali': 323,\n",
              " 'abdallah': 324,\n",
              " 'abdallar': 325,\n",
              " 'abdaly': 326,\n",
              " 'abdalyar': 327,\n",
              " 'abdel': 328,\n",
              " 'abdelaziz': 329,\n",
              " 'abdelbaset': 330,\n",
              " 'abdelkader': 331,\n",
              " 'abder': 332,\n",
              " 'abdghghsdgkagbdhfasdfashgfhasgfdgbhfgahgdfhasgfhasdgfhasgfhsdagfdsgahasgfsdagfsdhagfasdgasdgkfagskfgdakgaufgshfgasgfudysfgasfgdasfydagfyietfgyegfafgusageuawfgasytgfysgeygfuasdfgashtfgegfuyerfgrtfyiasfguyaefgyatfeyjgfuysdfgaetgfeygfasdfguywgasygfsuhafgerfyigdsgfuegfuydasguyfgeisgfdgfuyasyhgfilsdghlfksdlfghsdtfegyfdfaeraehfgldhksafhddhflhdasghfgyseraofhsbvu': 333,\n",
              " 'abdhir': 334,\n",
              " 'abdi': 335,\n",
              " 'abdicate': 336,\n",
              " 'abdicated': 337,\n",
              " 'abdication': 338,\n",
              " 'abdielcolberg': 339,\n",
              " 'abdillah': 340,\n",
              " 'abdin': 341,\n",
              " 'abdirahman': 342,\n",
              " 'abdirhman': 343,\n",
              " 'abdisalam': 344,\n",
              " 'abdnor': 345,\n",
              " 'abdolhassan': 346,\n",
              " 'abdolmalek': 347,\n",
              " 'abdomen': 348,\n",
              " 'abdominal': 349,\n",
              " 'abdoozy': 350,\n",
              " 'abdoreza': 351,\n",
              " 'abdou': 352,\n",
              " 'abdu': 353,\n",
              " 'abducted': 354,\n",
              " 'abductee': 355,\n",
              " 'abductees': 356,\n",
              " 'abducting': 357,\n",
              " 'abduction': 358,\n",
              " 'abductions': 359,\n",
              " 'abductive': 360,\n",
              " 'abdul': 361,\n",
              " 'abdulaziz': 362,\n",
              " 'abdulfattah': 363,\n",
              " 'abdulhakim': 364,\n",
              " 'abdulhamid': 365,\n",
              " 'abdulkadir': 366,\n",
              " 'abdulla': 367,\n",
              " 'abdullah': 368,\n",
              " 'abdullahi': 369,\n",
              " 'abdulqawi': 370,\n",
              " 'abdulrahman': 371,\n",
              " 'abdulrazak': 372,\n",
              " 'abdur': 373,\n",
              " 'abdurrahman': 374,\n",
              " 'abdus': 375,\n",
              " 'abdusamatov': 376,\n",
              " 'abe': 377,\n",
              " 'abebooks': 378,\n",
              " 'abecedare': 379,\n",
              " 'abecedarian': 380,\n",
              " 'abed': 381,\n",
              " 'abedin': 382,\n",
              " 'abeer': 383,\n",
              " 'abejingi': 384,\n",
              " 'abel': 385,\n",
              " 'abelard': 386,\n",
              " 'abele': 387,\n",
              " 'abeles': 388,\n",
              " 'abelian': 389,\n",
              " 'abelisaurs': 390,\n",
              " 'abella': 391,\n",
              " 'abelling': 392,\n",
              " 'abelson': 393,\n",
              " 'abenaki': 394,\n",
              " 'abend': 395,\n",
              " 'abendkl': 396,\n",
              " 'aber': 397,\n",
              " 'abercorn': 398,\n",
              " 'abercrombie': 399,\n",
              " 'abercromby': 400,\n",
              " 'aberdeen': 401,\n",
              " 'aberforth': 402,\n",
              " 'aberg': 403,\n",
              " 'abermaid': 404,\n",
              " 'abernathy': 405,\n",
              " 'abernethy': 406,\n",
              " 'aberoo': 407,\n",
              " 'aberrant': 408,\n",
              " 'aberrantly': 409,\n",
              " 'aberration': 410,\n",
              " 'aberrations': 411,\n",
              " 'abert': 412,\n",
              " 'aberthaw': 413,\n",
              " 'abertura': 414,\n",
              " 'abertzale': 415,\n",
              " 'abet': 416,\n",
              " 'abeter': 417,\n",
              " 'abett': 418,\n",
              " 'abetted': 419,\n",
              " 'abetting': 420,\n",
              " 'abettors': 421,\n",
              " 'abetz': 422,\n",
              " 'abewsively': 423,\n",
              " 'abey': 424,\n",
              " 'abeyance': 425,\n",
              " 'abf': 426,\n",
              " 'abgang': 427,\n",
              " 'abgar': 428,\n",
              " 'abgebremst': 429,\n",
              " 'abgegeben': 430,\n",
              " 'abgelten': 431,\n",
              " 'abgeschrieben': 432,\n",
              " 'abgezogen': 433,\n",
              " 'abgkagfv': 434,\n",
              " 'abh': 435,\n",
              " 'abhaga': 436,\n",
              " 'abhaile': 437,\n",
              " 'abhas': 438,\n",
              " 'abhay': 439,\n",
              " 'abhayagiri': 440,\n",
              " 'abhazia': 441,\n",
              " 'abheda': 442,\n",
              " 'abhi': 443,\n",
              " 'abhidhamma': 444,\n",
              " 'abhidharma': 445,\n",
              " 'abhijay': 446,\n",
              " 'abhijeet': 447,\n",
              " 'abhijna': 448,\n",
              " 'abhimanyu': 449,\n",
              " 'abhinav': 450,\n",
              " 'abhira': 451,\n",
              " 'abhiram': 452,\n",
              " 'abhiramdasji': 453,\n",
              " 'abhiras': 454,\n",
              " 'abhisamaya': 455,\n",
              " 'abhishek': 456,\n",
              " 'abhishekitm': 457,\n",
              " 'abhishikt': 458,\n",
              " 'abhisit': 459,\n",
              " 'abhor': 460,\n",
              " 'abhorant': 461,\n",
              " 'abhorred': 462,\n",
              " 'abhorrence': 463,\n",
              " 'abhorrent': 464,\n",
              " 'abhorrently': 465,\n",
              " 'abhorreo': 466,\n",
              " 'abhorridly': 467,\n",
              " 'abhors': 468,\n",
              " 'abhraham': 469,\n",
              " 'abhramidic': 470,\n",
              " 'abi': 471,\n",
              " 'abia': 472,\n",
              " 'abian': 473,\n",
              " 'abib': 474,\n",
              " 'abid': 475,\n",
              " 'abidal': 476,\n",
              " 'abide': 477,\n",
              " 'abided': 478,\n",
              " 'abideing': 479,\n",
              " 'abiders': 480,\n",
              " 'abides': 481,\n",
              " 'abidin': 482,\n",
              " 'abiding': 483,\n",
              " 'abidjan': 484,\n",
              " 'abie': 485,\n",
              " 'abientot': 486,\n",
              " 'abies': 487,\n",
              " 'abigail': 488,\n",
              " 'abigor': 489,\n",
              " 'abih': 490,\n",
              " 'abili': 491,\n",
              " 'abilites': 492,\n",
              " 'abilities': 493,\n",
              " 'ability': 494,\n",
              " 'abilitys': 495,\n",
              " 'abilty': 496,\n",
              " 'abimael': 497,\n",
              " 'abingdon': 498,\n",
              " 'abingdoni': 499,\n",
              " 'abington': 500,\n",
              " 'abiogenesis': 501,\n",
              " 'abiogenic': 502,\n",
              " 'abiogensis': 503,\n",
              " 'abiography': 504,\n",
              " 'abiotic': 505,\n",
              " 'abir': 506,\n",
              " 'abisharan': 507,\n",
              " 'abismo': 508,\n",
              " 'abisy': 509,\n",
              " 'abit': 510,\n",
              " 'abita': 511,\n",
              " 'abithiwtidb': 512,\n",
              " 'abitrary': 513,\n",
              " 'abitrators': 514,\n",
              " 'abitur': 515,\n",
              " 'abiyebudur': 516,\n",
              " 'abjads': 517,\n",
              " 'abject': 518,\n",
              " 'abjectio': 519,\n",
              " 'abjectives': 520,\n",
              " 'abjectly': 521,\n",
              " 'abjihad': 522,\n",
              " 'abk': 523,\n",
              " 'abkahzia': 524,\n",
              " 'abkahzians': 525,\n",
              " 'abkaz': 526,\n",
              " 'abkazia': 527,\n",
              " 'abkazians': 528,\n",
              " 'abkco': 529,\n",
              " 'abkhaz': 530,\n",
              " 'abkhazia': 531,\n",
              " 'abkhazian': 532,\n",
              " 'abkhazians': 533,\n",
              " 'abl': 534,\n",
              " 'ablag': 535,\n",
              " 'ablan': 536,\n",
              " 'ablation': 537,\n",
              " 'ablaut': 538,\n",
              " 'ablaze': 539,\n",
              " 'able': 540,\n",
              " 'ableben': 541,\n",
              " 'abled': 542,\n",
              " 'ableist': 543,\n",
              " 'ableiten': 544,\n",
              " 'abler': 545,\n",
              " 'ableson': 546,\n",
              " 'ableton': 547,\n",
              " 'ablichtung': 548,\n",
              " 'ablilty': 549,\n",
              " 'ablindinglight': 550,\n",
              " 'ablum': 551,\n",
              " 'ablution': 552,\n",
              " 'ablutions': 553,\n",
              " 'ably': 554,\n",
              " 'abm': 555,\n",
              " 'abmin': 556,\n",
              " 'abn': 557,\n",
              " 'abnegation': 558,\n",
              " 'abnett': 559,\n",
              " 'abney': 560,\n",
              " 'abnima': 561,\n",
              " 'abnor': 562,\n",
              " 'abnormal': 563,\n",
              " 'abnormalities': 564,\n",
              " 'abnormality': 565,\n",
              " 'abnormally': 566,\n",
              " 'abnormals': 567,\n",
              " 'abnout': 568,\n",
              " 'abo': 569,\n",
              " 'aboard': 570,\n",
              " 'abode': 571,\n",
              " 'abogados': 572,\n",
              " 'aboiut': 573,\n",
              " 'abokor': 574,\n",
              " 'abolamation': 575,\n",
              " 'abolish': 576,\n",
              " 'abolished': 577,\n",
              " 'abolishing': 578,\n",
              " 'abolishion': 579,\n",
              " 'abolishment': 580,\n",
              " 'abolition': 581,\n",
              " 'abolitionism': 582,\n",
              " 'abolitionist': 583,\n",
              " 'abolitionists': 584,\n",
              " 'abolsutely': 585,\n",
              " 'aboltion': 586,\n",
              " 'abolute': 587,\n",
              " 'abolutely': 588,\n",
              " 'abolution': 589,\n",
              " 'abomasum': 590,\n",
              " 'abominable': 591,\n",
              " 'abominably': 592,\n",
              " 'abomination': 593,\n",
              " 'abominations': 594,\n",
              " 'abondantly': 595,\n",
              " 'abondoned': 596,\n",
              " 'abook': 597,\n",
              " 'aboot': 598,\n",
              " 'aboout': 599,\n",
              " 'aboput': 600,\n",
              " 'abor': 601,\n",
              " 'aborbs': 602,\n",
              " 'aborginals': 603,\n",
              " 'aborigin': 604,\n",
              " 'aboriginal': 605,\n",
              " 'aboriginals': 606,\n",
              " 'aborigine': 607,\n",
              " 'aborigines': 608,\n",
              " 'aborigins': 609,\n",
              " 'aborn': 610,\n",
              " 'aborogate': 611,\n",
              " 'abort': 612,\n",
              " 'aborted': 613,\n",
              " 'abortifacient': 614,\n",
              " 'abortifacients': 615,\n",
              " 'abortificant': 616,\n",
              " 'aborting': 617,\n",
              " 'abortion': 618,\n",
              " 'abortionist': 619,\n",
              " 'abortionists': 620,\n",
              " 'abortions': 621,\n",
              " 'abortive': 622,\n",
              " 'aborz': 623,\n",
              " 'abos': 624,\n",
              " 'aboslute': 625,\n",
              " 'aboslutly': 626,\n",
              " 'abosolut': 627,\n",
              " 'abosolute': 628,\n",
              " 'abosulte': 629,\n",
              " 'abosultely': 630,\n",
              " 'abosutly': 631,\n",
              " 'abot': 632,\n",
              " 'abotaged': 633,\n",
              " 'abother': 634,\n",
              " 'abotu': 635,\n",
              " 'abou': 636,\n",
              " 'aboud': 637,\n",
              " 'aboulker': 638,\n",
              " 'abound': 639,\n",
              " 'abounding': 640,\n",
              " 'abounds': 641,\n",
              " 'abount': 642,\n",
              " 'abour': 643,\n",
              " 'abourezk': 644,\n",
              " 'about': 645,\n",
              " 'abouta': 646,\n",
              " 'aboutcivil': 647,\n",
              " 'abouth': 648,\n",
              " 'abouthosur': 649,\n",
              " 'aboutir': 650,\n",
              " 'aboutmovies': 651,\n",
              " 'abouto': 652,\n",
              " 'abouts': 653,\n",
              " 'aboutself': 654,\n",
              " 'aboutthat': 655,\n",
              " 'aboutthe': 656,\n",
              " 'aboutus': 657,\n",
              " 'aboutv': 658,\n",
              " 'aboutwikibooks': 659,\n",
              " 'aboutwikipedia': 660,\n",
              " 'above': 661,\n",
              " 'aboveand': 662,\n",
              " 'aboveboard': 663,\n",
              " 'aboveground': 664,\n",
              " 'abovementioned': 665,\n",
              " 'aboverely': 666,\n",
              " 'abovian': 667,\n",
              " 'abow': 668,\n",
              " 'abp': 669,\n",
              " 'abpout': 670,\n",
              " 'abput': 671,\n",
              " 'abr': 672,\n",
              " 'abra': 673,\n",
              " 'abracadabra': 674,\n",
              " 'abragam': 675,\n",
              " 'abraha': 676,\n",
              " 'abraham': 677,\n",
              " 'abrahamcoapman': 678,\n",
              " 'abrahamdavidson': 679,\n",
              " 'abrahamic': 680,\n",
              " 'abrahamisaac': 681,\n",
              " 'abrahamlicon': 682,\n",
              " 'abrahams': 683,\n",
              " 'abrahamsson': 684,\n",
              " 'abrahmic': 685,\n",
              " 'abrahms': 686,\n",
              " 'abram': 687,\n",
              " 'abramce': 688,\n",
              " 'abramites': 689,\n",
              " 'abramoff': 690,\n",
              " 'abramoffrefactor': 691,\n",
              " 'abramowitsch': 692,\n",
              " 'abrams': 693,\n",
              " 'abramsky': 694,\n",
              " 'abramson': 695,\n",
              " 'abrar': 696,\n",
              " 'abras': 697,\n",
              " 'abrasion': 698,\n",
              " 'abrasive': 699,\n",
              " 'abrasiveness': 700,\n",
              " 'abrasives': 701,\n",
              " 'abratenaxis': 702,\n",
              " 'abrazame': 703,\n",
              " 'abrazo': 704,\n",
              " 'abre': 705,\n",
              " 'abreast': 706,\n",
              " 'abrech': 707,\n",
              " 'abrera': 708,\n",
              " 'abreu': 709,\n",
              " 'abrevashun': 710,\n",
              " 'abreviated': 711,\n",
              " 'abreviation': 712,\n",
              " 'abreviations': 713,\n",
              " 'abrham': 714,\n",
              " 'abri': 715,\n",
              " 'abridge': 716,\n",
              " 'abridged': 717,\n",
              " 'abridgement': 718,\n",
              " 'abridging': 719,\n",
              " 'abrieb': 720,\n",
              " 'abriendo': 721,\n",
              " 'abrievenbus': 722,\n",
              " 'abril': 723,\n",
              " 'abrimori': 724,\n",
              " 'abrir': 725,\n",
              " 'abritation': 726,\n",
              " 'abritish': 727,\n",
              " 'abritration': 728,\n",
              " 'abritrators': 729,\n",
              " 'abriviation': 730,\n",
              " 'abroad': 731,\n",
              " 'abrogated': 732,\n",
              " 'abrogating': 733,\n",
              " 'abrogation': 734,\n",
              " 'abroken': 735,\n",
              " 'abrolhos': 736,\n",
              " 'abrowng': 737,\n",
              " 'abrubt': 738,\n",
              " 'abrupt': 739,\n",
              " 'abruptive': 740,\n",
              " 'abruptly': 741,\n",
              " 'abruptness': 742,\n",
              " 'abs': 743,\n",
              " 'absalom': 744,\n",
              " 'absame': 745,\n",
              " 'absander': 746,\n",
              " 'absarat': 747,\n",
              " 'abscence': 748,\n",
              " 'abscess': 749,\n",
              " 'abschnitt': 750,\n",
              " 'abschnittsburg': 751,\n",
              " 'abschreiber': 752,\n",
              " 'abscissa': 753,\n",
              " 'abscom': 754,\n",
              " 'absconder': 755,\n",
              " 'absconders': 756,\n",
              " 'absence': 757,\n",
              " 'absences': 758,\n",
              " 'absense': 759,\n",
              " 'absent': 760,\n",
              " 'absentbe': 761,\n",
              " 'absentee': 762,\n",
              " 'absentees': 763,\n",
              " 'absentia': 764,\n",
              " 'absently': 765,\n",
              " 'abservers': 766,\n",
              " 'absetzung': 767,\n",
              " 'absgroup': 768,\n",
              " 'absicht': 769,\n",
              " 'absicllay': 770,\n",
              " 'absidy': 771,\n",
              " 'absinthe': 772,\n",
              " 'absitnence': 773,\n",
              " 'absmag': 774,\n",
              " 'absofreakinlutely': 775,\n",
              " 'absoft': 776,\n",
              " 'absofuckinglutely': 777,\n",
              " 'absolkutely': 778,\n",
              " 'absololutely': 779,\n",
              " 'absolom': 780,\n",
              " 'absoloute': 781,\n",
              " 'absoloutely': 782,\n",
              " 'absoloutly': 783,\n",
              " 'absolte': 784,\n",
              " 'absoltuely': 785,\n",
              " 'absoluetly': 786,\n",
              " 'absolument': 787,\n",
              " 'absolut': 788,\n",
              " 'absolutas': 789,\n",
              " 'absolutdan': 790,\n",
              " 'absolute': 791,\n",
              " 'absoluteastronomy': 792,\n",
              " 'absolutedan': 793,\n",
              " 'absolutelly': 794,\n",
              " 'absolutely': 795,\n",
              " 'absolutemadonna': 796,\n",
              " 'absoluter': 797,\n",
              " 'absoluterock': 798,\n",
              " 'absolutes': 799,\n",
              " 'absolutewrite': 800,\n",
              " 'absolution': 801,\n",
              " 'absolutisht': 802,\n",
              " 'absolutism': 803,\n",
              " 'absolutist': 804,\n",
              " 'absolutive': 805,\n",
              " 'absolutley': 806,\n",
              " 'absolutly': 807,\n",
              " 'absoluto': 808,\n",
              " 'absolutte': 809,\n",
              " 'absolve': 810,\n",
              " 'absolved': 811,\n",
              " 'absolves': 812,\n",
              " 'absolving': 813,\n",
              " 'absorb': 814,\n",
              " 'absorbably': 815,\n",
              " 'absorbed': 816,\n",
              " 'absorbency': 817,\n",
              " 'absorbend': 818,\n",
              " 'absorbent': 819,\n",
              " 'absorbents': 820,\n",
              " 'absorbers': 821,\n",
              " 'absorbing': 822,\n",
              " 'absorbs': 823,\n",
              " 'absorbtion': 824,\n",
              " 'absorption': 825,\n",
              " 'absortion': 826,\n",
              " 'absouloulty': 827,\n",
              " 'absoultely': 828,\n",
              " 'absoultlly': 829,\n",
              " 'absoulute': 830,\n",
              " 'absoulutly': 831,\n",
              " 'absoutely': 832,\n",
              " 'abstact': 833,\n",
              " 'abstain': 834,\n",
              " 'abstained': 835,\n",
              " 'abstaining': 836,\n",
              " 'abstains': 837,\n",
              " 'abstammen': 838,\n",
              " 'abstance': 839,\n",
              " 'abstandsprache': 840,\n",
              " 'abstensions': 841,\n",
              " 'abstentia': 842,\n",
              " 'abstention': 843,\n",
              " 'abstentionism': 844,\n",
              " 'abstentionist': 845,\n",
              " 'abstentions': 846,\n",
              " 'abstinant': 847,\n",
              " 'abstinence': 848,\n",
              " 'abstract': 849,\n",
              " 'abstracta': 850,\n",
              " 'abstracted': 851,\n",
              " 'abstracting': 852,\n",
              " 'abstraction': 853,\n",
              " 'abstractionism': 854,\n",
              " 'abstractionor': 855,\n",
              " 'abstractions': 856,\n",
              " 'abstractly': 857,\n",
              " 'abstracts': 858,\n",
              " 'abstrakt': 859,\n",
              " 'abstruse': 860,\n",
              " 'absue': 861,\n",
              " 'absued': 862,\n",
              " 'absuing': 863,\n",
              " 'absulotly': 864,\n",
              " 'absurb': 865,\n",
              " 'absurd': 866,\n",
              " 'absurdem': 867,\n",
              " 'absurdist': 868,\n",
              " 'absurdities': 869,\n",
              " 'absurdity': 870,\n",
              " 'absurdly': 871,\n",
              " 'absurdness': 872,\n",
              " 'absurdum': 873,\n",
              " 'absurt': 874,\n",
              " 'abt': 875,\n",
              " 'abtalion': 876,\n",
              " 'abtalk': 877,\n",
              " 'abtc': 878,\n",
              " 'abtibiotics': 879,\n",
              " 'abtin': 880,\n",
              " 'abtract': 881,\n",
              " 'abtracts': 882,\n",
              " 'abu': 883,\n",
              " 'abubu': 884,\n",
              " 'abudctive': 885,\n",
              " 'abudllah': 886,\n",
              " 'abuela': 887,\n",
              " 'abuelo': 888,\n",
              " 'abueses': 889,\n",
              " 'abugida': 890,\n",
              " 'abugidas': 891,\n",
              " 'abugoush': 892,\n",
              " 'abuisve': 893,\n",
              " 'abuja': 894,\n",
              " 'abujihad': 895,\n",
              " 'abul': 896,\n",
              " 'abulhawa': 897,\n",
              " 'abulkamal': 898,\n",
              " 'abunch': 899,\n",
              " 'abuncha': 900,\n",
              " 'abundance': 901,\n",
              " 'abundant': 902,\n",
              " 'abundantly': 903,\n",
              " 'abundently': 904,\n",
              " 'abuot': 905,\n",
              " 'aburaneh': 906,\n",
              " 'abureem': 907,\n",
              " 'abus': 908,\n",
              " 'abusaid': 909,\n",
              " 'abuse': 910,\n",
              " 'abusean': 911,\n",
              " 'abused': 912,\n",
              " 'abusefilter': 913,\n",
              " 'abuseive': 914,\n",
              " 'abusephilter': 915,\n",
              " 'abuser': 916,\n",
              " 'abusers': 917,\n",
              " 'abuses': 918,\n",
              " 'abusi': 919,\n",
              " 'abusing': 920,\n",
              " 'abusinmg': 921,\n",
              " 'abusive': 922,\n",
              " 'abusively': 923,\n",
              " 'abusiveness': 924,\n",
              " 'abusivesockpuppetry': 925,\n",
              " 'abusivwmf': 926,\n",
              " 'abuso': 927,\n",
              " 'abusrd': 928,\n",
              " 'abusse': 929,\n",
              " 'abusu': 930,\n",
              " 'abusuing': 931,\n",
              " 'abusus': 932,\n",
              " 'abut': 933,\n",
              " 'abutted': 934,\n",
              " 'abutting': 935,\n",
              " 'abuzz': 936,\n",
              " 'abv': 937,\n",
              " 'abvice': 938,\n",
              " 'abvious': 939,\n",
              " 'abvoe': 940,\n",
              " 'abw': 941,\n",
              " 'abwaanno': 942,\n",
              " 'abwanderung': 943,\n",
              " 'abwehr': 944,\n",
              " 'abwertend': 945,\n",
              " 'abwher': 946,\n",
              " 'abwinken': 947,\n",
              " 'abx': 948,\n",
              " 'aby': 949,\n",
              " 'abyad': 950,\n",
              " 'abyslam': 951,\n",
              " 'abysmal': 952,\n",
              " 'abysmally': 953,\n",
              " 'abyss': 954,\n",
              " 'abyssal': 955,\n",
              " 'abyssinia': 956,\n",
              " 'abyssinians': 957,\n",
              " 'abyssus': 958,\n",
              " 'abzubauen': 959,\n",
              " 'abzug': 960,\n",
              " 'ac': 961,\n",
              " 'aca': 962,\n",
              " 'acabashi': 963,\n",
              " 'acabo': 964,\n",
              " 'acacia': 965,\n",
              " 'acacias': 966,\n",
              " 'acad': 967,\n",
              " 'acadamey': 968,\n",
              " 'acadamic': 969,\n",
              " 'acadamy': 970,\n",
              " 'academc': 971,\n",
              " 'academe': 972,\n",
              " 'academi': 973,\n",
              " 'academia': 974,\n",
              " 'academiaif': 975,\n",
              " 'academic': 976,\n",
              " 'academicae': 977,\n",
              " 'academical': 978,\n",
              " 'academically': 979,\n",
              " 'academician': 980,\n",
              " 'academicians': 981,\n",
              " 'academics': 982,\n",
              " 'academie': 983,\n",
              " 'academies': 984,\n",
              " 'academiia': 985,\n",
              " 'academin': 986,\n",
              " 'academis': 987,\n",
              " 'academitions': 988,\n",
              " 'academize': 989,\n",
              " 'academny': 990,\n",
              " 'academy': 991,\n",
              " 'acadia': 992,\n",
              " 'acadiahere': 993,\n",
              " 'acadian': 994,\n",
              " 'acadians': 995,\n",
              " 'acadiens': 996,\n",
              " 'acadimay': 997,\n",
              " 'acadmemey': 998,\n",
              " 'acadmic': 999,\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A column is created with same list of words but this time coded with numbers\n",
        "def coded_words(x,word_dict):\n",
        "    return [word_dict[w] for w in x if w in word_dict]"
      ],
      "metadata": {
        "id": "IZ4prD5E_cL_"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['coded'] = train_df['splitted'].apply(lambda x: coded_words(x,word_dict))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kL1dA2BbbLri",
        "outputId": "252c2329-a47a-4fbc-c17b-2767534fdb73"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-25-5c53e8ac5d18>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_df['coded'] = train_df['splitted'].apply(lambda x: coded_words(x,word_dict))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df"
      ],
      "metadata": {
        "id": "wYuVyrwd5cjN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "outputId": "6bca6b88-0a9a-4ee4-b5c6-b6e6bc1dc470"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                      id                                       comment_text  \\\n",
              "0       0000997932d777bf  explanation why the edits made under my userna...   \n",
              "1       000103f0d9cfb60f  d aww he matches this background colour i am s...   \n",
              "2       000113f07ec002fd  hey man i am really not trying to edit war it ...   \n",
              "3       0001b41b1c6bb37e  more i cannot make any real suggestions on imp...   \n",
              "4       0001d958c54c6e35  you sir are my hero any chance you remember wh...   \n",
              "...                  ...                                                ...   \n",
              "223544  fff8f64043129fa2  jerome i see you never got around to this i m ...   \n",
              "223545  fff9d70fe0722906  lucky bastard heh you are famous now i kida en...   \n",
              "223546  fffa8a11c4378854  shame on you all you want to speak about gays ...   \n",
              "223547  fffac2a094c8e0e2  mel gibson is a nazi bitch who makes shitty mo...   \n",
              "223548  fffb5451268fb5ba  unicorn lair discovery supposedly a unicorn la...   \n",
              "\n",
              "        toxic  severe_toxic  obscene  threat  insult  identity_hate   type  \\\n",
              "0           0             0        0       0       0              0  train   \n",
              "1           0             0        0       0       0              0  train   \n",
              "2           0             0        0       0       0              0  train   \n",
              "3           0             0        0       0       0              0  train   \n",
              "4           0             0        0       0       0              0  train   \n",
              "...       ...           ...      ...     ...     ...            ...    ...   \n",
              "223544      0             0        0       0       0              0   test   \n",
              "223545      0             0        0       0       0              0   test   \n",
              "223546      0             0        0       0       0              0   test   \n",
              "223547      1             0        1       0       1              0   test   \n",
              "223548      0             0        0       0       0              0   test   \n",
              "\n",
              "                                                 splitted  len  \\\n",
              "0       [explanation, why, the, edits, made, under, my...   44   \n",
              "1       [aww, he, matches, this, background, colour, a...   14   \n",
              "2       [hey, man, am, really, not, trying, to, edit, ...   42   \n",
              "3       [more, cannot, make, any, real, suggestions, o...  105   \n",
              "4       [you, sir, are, my, hero, any, chance, you, re...   13   \n",
              "...                                                   ...  ...   \n",
              "223544  [jerome, see, you, never, got, around, to, thi...  101   \n",
              "223545  [lucky, bastard, heh, you, are, famous, now, k...   11   \n",
              "223546  [shame, on, you, all, you, want, to, speak, ab...   14   \n",
              "223547  [mel, gibson, is, nazi, bitch, who, makes, shi...   25   \n",
              "223548  [unicorn, lair, discovery, supposedly, unicorn...   39   \n",
              "\n",
              "                                                    coded  \n",
              "0       [62365, 205816, 186656, 55444, 111408, 195291,...  \n",
              "1       [14338, 80784, 114535, 187579, 14924, 35721, 6...  \n",
              "2       [82303, 112707, 6225, 153670, 129959, 192451, ...  \n",
              "3       [121816, 28011, 112173, 8798, 153588, 180313, ...  \n",
              "4       [211069, 171270, 10398, 124141, 82021, 8798, 3...  \n",
              "...                                                   ...  \n",
              "223544  [95471, 166338, 211069, 127341, 75620, 10971, ...  \n",
              "223545  [110315, 16875, 81229, 211069, 10398, 63747, 1...  \n",
              "223546  [168278, 133374, 211069, 5424, 211069, 203435,...  \n",
              "223547  [116523, 73578, 92901, 125908, 20894, 205675, ...  \n",
              "223548  [196053, 104443, 49955, 181210, 196053, 104443...  \n",
              "\n",
              "[223050 rows x 12 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a6a185cc-9088-4c07-be3c-16d5aa4961cd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "      <th>type</th>\n",
              "      <th>splitted</th>\n",
              "      <th>len</th>\n",
              "      <th>coded</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0000997932d777bf</td>\n",
              "      <td>explanation why the edits made under my userna...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "      <td>[explanation, why, the, edits, made, under, my...</td>\n",
              "      <td>44</td>\n",
              "      <td>[62365, 205816, 186656, 55444, 111408, 195291,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000103f0d9cfb60f</td>\n",
              "      <td>d aww he matches this background colour i am s...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "      <td>[aww, he, matches, this, background, colour, a...</td>\n",
              "      <td>14</td>\n",
              "      <td>[14338, 80784, 114535, 187579, 14924, 35721, 6...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>000113f07ec002fd</td>\n",
              "      <td>hey man i am really not trying to edit war it ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "      <td>[hey, man, am, really, not, trying, to, edit, ...</td>\n",
              "      <td>42</td>\n",
              "      <td>[82303, 112707, 6225, 153670, 129959, 192451, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0001b41b1c6bb37e</td>\n",
              "      <td>more i cannot make any real suggestions on imp...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "      <td>[more, cannot, make, any, real, suggestions, o...</td>\n",
              "      <td>105</td>\n",
              "      <td>[121816, 28011, 112173, 8798, 153588, 180313, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0001d958c54c6e35</td>\n",
              "      <td>you sir are my hero any chance you remember wh...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "      <td>[you, sir, are, my, hero, any, chance, you, re...</td>\n",
              "      <td>13</td>\n",
              "      <td>[211069, 171270, 10398, 124141, 82021, 8798, 3...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>223544</th>\n",
              "      <td>fff8f64043129fa2</td>\n",
              "      <td>jerome i see you never got around to this i m ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>test</td>\n",
              "      <td>[jerome, see, you, never, got, around, to, thi...</td>\n",
              "      <td>101</td>\n",
              "      <td>[95471, 166338, 211069, 127341, 75620, 10971, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>223545</th>\n",
              "      <td>fff9d70fe0722906</td>\n",
              "      <td>lucky bastard heh you are famous now i kida en...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>test</td>\n",
              "      <td>[lucky, bastard, heh, you, are, famous, now, k...</td>\n",
              "      <td>11</td>\n",
              "      <td>[110315, 16875, 81229, 211069, 10398, 63747, 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>223546</th>\n",
              "      <td>fffa8a11c4378854</td>\n",
              "      <td>shame on you all you want to speak about gays ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>test</td>\n",
              "      <td>[shame, on, you, all, you, want, to, speak, ab...</td>\n",
              "      <td>14</td>\n",
              "      <td>[168278, 133374, 211069, 5424, 211069, 203435,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>223547</th>\n",
              "      <td>fffac2a094c8e0e2</td>\n",
              "      <td>mel gibson is a nazi bitch who makes shitty mo...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>test</td>\n",
              "      <td>[mel, gibson, is, nazi, bitch, who, makes, shi...</td>\n",
              "      <td>25</td>\n",
              "      <td>[116523, 73578, 92901, 125908, 20894, 205675, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>223548</th>\n",
              "      <td>fffb5451268fb5ba</td>\n",
              "      <td>unicorn lair discovery supposedly a unicorn la...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>test</td>\n",
              "      <td>[unicorn, lair, discovery, supposedly, unicorn...</td>\n",
              "      <td>39</td>\n",
              "      <td>[196053, 104443, 49955, 181210, 196053, 104443...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>223050 rows × 12 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a6a185cc-9088-4c07-be3c-16d5aa4961cd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a6a185cc-9088-4c07-be3c-16d5aa4961cd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a6a185cc-9088-4c07-be3c-16d5aa4961cd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configure train and test set"
      ],
      "metadata": {
        "id": "KdpXYRwXq2y-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 10-column embed model\n",
        "embedding = nn.Embedding(len(word_dict), 10)"
      ],
      "metadata": {
        "id": "onCId7Za_mBu"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's filter train and test again\n",
        "df_train = train_df[train_df['type']=='train']\n",
        "df_test = train_df[train_df['type']=='test']"
      ],
      "metadata": {
        "id": "TkhCGXVl5_ZJ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get labels of each group\n",
        "Y_train = torch.tensor(torch.tensor(df_train[df_train.columns[2:8]].to_numpy()))\n",
        "Y_test = torch.tensor(torch.tensor(df_test[df_test.columns[2:8]].to_numpy()))"
      ],
      "metadata": {
        "id": "419kSMAaLBtV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ab19d12-027e-4bec-8f79-3718be74e3a6"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-29-fae62f09e3e5>:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  Y_train = torch.tensor(torch.tensor(df_train[df_train.columns[2:8]].to_numpy()))\n",
            "<ipython-input-29-fae62f09e3e5>:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  Y_test = torch.tensor(torch.tensor(df_test[df_test.columns[2:8]].to_numpy()))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_test"
      ],
      "metadata": {
        "id": "-dM7MBIJfhAK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "468b3b92-53b5-4932-ecee-ab81455c3d7f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [1, 0, 1, 0, 1, 0],\n",
              "        [0, 0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Embed columns are created\n",
        "df_train['vect_tensor'] = df_train['coded'].apply(lambda x: embedding(torch.tensor(x).long()))\n",
        "df_test['vect_tensor'] = df_test['coded'].apply(lambda x: embedding(torch.tensor(x).long()))"
      ],
      "metadata": {
        "id": "Gy6CC0P_NXYW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "248839d5-8bb0-4e1b-b82e-262b1124c7ae"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-31-2aece187f108>:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_train['vect_tensor'] = df_train['coded'].apply(lambda x: embedding(torch.tensor(x).long()))\n",
            "<ipython-input-31-2aece187f108>:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_test['vect_tensor'] = df_test['coded'].apply(lambda x: embedding(torch.tensor(x).long()))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test"
      ],
      "metadata": {
        "id": "-X6E_wS1gFGj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 815
        },
        "outputId": "feb390e4-101e-4f35-8657-1c651b247132"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                      id                                       comment_text  \\\n",
              "159571  0001ea8717f6de06  thank you for understanding i think very highl...   \n",
              "159572  000247e83dcc1211                     dear god this site is horrible   \n",
              "159573  0002f87b16116a7f  somebody will invariably try to add religion r...   \n",
              "159574  0003e1cccfd5a40a  it says it right there that it is a type the t...   \n",
              "159575  00059ace3e3e9a53  before adding a new product to the list make s...   \n",
              "...                  ...                                                ...   \n",
              "223544  fff8f64043129fa2  jerome i see you never got around to this i m ...   \n",
              "223545  fff9d70fe0722906  lucky bastard heh you are famous now i kida en...   \n",
              "223546  fffa8a11c4378854  shame on you all you want to speak about gays ...   \n",
              "223547  fffac2a094c8e0e2  mel gibson is a nazi bitch who makes shitty mo...   \n",
              "223548  fffb5451268fb5ba  unicorn lair discovery supposedly a unicorn la...   \n",
              "\n",
              "        toxic  severe_toxic  obscene  threat  insult  identity_hate  type  \\\n",
              "159571      0             0        0       0       0              0  test   \n",
              "159572      0             0        0       0       0              0  test   \n",
              "159573      0             0        0       0       0              0  test   \n",
              "159574      0             0        0       0       0              0  test   \n",
              "159575      0             0        0       0       0              0  test   \n",
              "...       ...           ...      ...     ...     ...            ...   ...   \n",
              "223544      0             0        0       0       0              0  test   \n",
              "223545      0             0        0       0       0              0  test   \n",
              "223546      0             0        0       0       0              0  test   \n",
              "223547      1             0        1       0       1              0  test   \n",
              "223548      0             0        0       0       0              0  test   \n",
              "\n",
              "                                                 splitted  len  \\\n",
              "159571  [thank, you, for, understanding, think, very, ...   15   \n",
              "159572              [dear, god, this, site, is, horrible]    6   \n",
              "159573  [somebody, will, invariably, try, to, add, rel...   69   \n",
              "159574  [it, says, it, right, there, that, it, is, typ...   81   \n",
              "159575  [before, adding, new, product, to, the, list, ...   51   \n",
              "...                                                   ...  ...   \n",
              "223544  [jerome, see, you, never, got, around, to, thi...  101   \n",
              "223545  [lucky, bastard, heh, you, are, famous, now, k...   11   \n",
              "223546  [shame, on, you, all, you, want, to, speak, ab...   14   \n",
              "223547  [mel, gibson, is, nazi, bitch, who, makes, shi...   25   \n",
              "223548  [unicorn, lair, discovery, supposedly, unicorn...   39   \n",
              "\n",
              "                                                    coded  \\\n",
              "159571  [186525, 211069, 67795, 195460, 187476, 200636...   \n",
              "159572       [44625, 74791, 187579, 171388, 92901, 84655]   \n",
              "159573  [174049, 207037, 92111, 192434, 189276, 1993, ...   \n",
              "159574  [93472, 164338, 93472, 158979, 187099, 186592,...   \n",
              "159575  [17994, 2044, 127382, 147905, 189276, 186656, ...   \n",
              "...                                                   ...   \n",
              "223544  [95471, 166338, 211069, 127341, 75620, 10971, ...   \n",
              "223545  [110315, 16875, 81229, 211069, 10398, 63747, 1...   \n",
              "223546  [168278, 133374, 211069, 5424, 211069, 203435,...   \n",
              "223547  [116523, 73578, 92901, 125908, 20894, 205675, ...   \n",
              "223548  [196053, 104443, 49955, 181210, 196053, 104443...   \n",
              "\n",
              "                                              vect_tensor  \n",
              "159571  [[tensor(0.8067, grad_fn=<UnbindBackward0>), t...  \n",
              "159572  [[tensor(0.8508, grad_fn=<UnbindBackward0>), t...  \n",
              "159573  [[tensor(0.4611, grad_fn=<UnbindBackward0>), t...  \n",
              "159574  [[tensor(-0.7552, grad_fn=<UnbindBackward0>), ...  \n",
              "159575  [[tensor(-0.2818, grad_fn=<UnbindBackward0>), ...  \n",
              "...                                                   ...  \n",
              "223544  [[tensor(0.7361, grad_fn=<UnbindBackward0>), t...  \n",
              "223545  [[tensor(0.8081, grad_fn=<UnbindBackward0>), t...  \n",
              "223546  [[tensor(0.2971, grad_fn=<UnbindBackward0>), t...  \n",
              "223547  [[tensor(1.3340, grad_fn=<UnbindBackward0>), t...  \n",
              "223548  [[tensor(0.5316, grad_fn=<UnbindBackward0>), t...  \n",
              "\n",
              "[63513 rows x 13 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-13fb864d-7190-4d5c-b7ad-d20c0a9562f2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "      <th>type</th>\n",
              "      <th>splitted</th>\n",
              "      <th>len</th>\n",
              "      <th>coded</th>\n",
              "      <th>vect_tensor</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>159571</th>\n",
              "      <td>0001ea8717f6de06</td>\n",
              "      <td>thank you for understanding i think very highl...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>test</td>\n",
              "      <td>[thank, you, for, understanding, think, very, ...</td>\n",
              "      <td>15</td>\n",
              "      <td>[186525, 211069, 67795, 195460, 187476, 200636...</td>\n",
              "      <td>[[tensor(0.8067, grad_fn=&lt;UnbindBackward0&gt;), t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159572</th>\n",
              "      <td>000247e83dcc1211</td>\n",
              "      <td>dear god this site is horrible</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>test</td>\n",
              "      <td>[dear, god, this, site, is, horrible]</td>\n",
              "      <td>6</td>\n",
              "      <td>[44625, 74791, 187579, 171388, 92901, 84655]</td>\n",
              "      <td>[[tensor(0.8508, grad_fn=&lt;UnbindBackward0&gt;), t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159573</th>\n",
              "      <td>0002f87b16116a7f</td>\n",
              "      <td>somebody will invariably try to add religion r...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>test</td>\n",
              "      <td>[somebody, will, invariably, try, to, add, rel...</td>\n",
              "      <td>69</td>\n",
              "      <td>[174049, 207037, 92111, 192434, 189276, 1993, ...</td>\n",
              "      <td>[[tensor(0.4611, grad_fn=&lt;UnbindBackward0&gt;), t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159574</th>\n",
              "      <td>0003e1cccfd5a40a</td>\n",
              "      <td>it says it right there that it is a type the t...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>test</td>\n",
              "      <td>[it, says, it, right, there, that, it, is, typ...</td>\n",
              "      <td>81</td>\n",
              "      <td>[93472, 164338, 93472, 158979, 187099, 186592,...</td>\n",
              "      <td>[[tensor(-0.7552, grad_fn=&lt;UnbindBackward0&gt;), ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159575</th>\n",
              "      <td>00059ace3e3e9a53</td>\n",
              "      <td>before adding a new product to the list make s...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>test</td>\n",
              "      <td>[before, adding, new, product, to, the, list, ...</td>\n",
              "      <td>51</td>\n",
              "      <td>[17994, 2044, 127382, 147905, 189276, 186656, ...</td>\n",
              "      <td>[[tensor(-0.2818, grad_fn=&lt;UnbindBackward0&gt;), ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>223544</th>\n",
              "      <td>fff8f64043129fa2</td>\n",
              "      <td>jerome i see you never got around to this i m ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>test</td>\n",
              "      <td>[jerome, see, you, never, got, around, to, thi...</td>\n",
              "      <td>101</td>\n",
              "      <td>[95471, 166338, 211069, 127341, 75620, 10971, ...</td>\n",
              "      <td>[[tensor(0.7361, grad_fn=&lt;UnbindBackward0&gt;), t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>223545</th>\n",
              "      <td>fff9d70fe0722906</td>\n",
              "      <td>lucky bastard heh you are famous now i kida en...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>test</td>\n",
              "      <td>[lucky, bastard, heh, you, are, famous, now, k...</td>\n",
              "      <td>11</td>\n",
              "      <td>[110315, 16875, 81229, 211069, 10398, 63747, 1...</td>\n",
              "      <td>[[tensor(0.8081, grad_fn=&lt;UnbindBackward0&gt;), t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>223546</th>\n",
              "      <td>fffa8a11c4378854</td>\n",
              "      <td>shame on you all you want to speak about gays ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>test</td>\n",
              "      <td>[shame, on, you, all, you, want, to, speak, ab...</td>\n",
              "      <td>14</td>\n",
              "      <td>[168278, 133374, 211069, 5424, 211069, 203435,...</td>\n",
              "      <td>[[tensor(0.2971, grad_fn=&lt;UnbindBackward0&gt;), t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>223547</th>\n",
              "      <td>fffac2a094c8e0e2</td>\n",
              "      <td>mel gibson is a nazi bitch who makes shitty mo...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>test</td>\n",
              "      <td>[mel, gibson, is, nazi, bitch, who, makes, shi...</td>\n",
              "      <td>25</td>\n",
              "      <td>[116523, 73578, 92901, 125908, 20894, 205675, ...</td>\n",
              "      <td>[[tensor(1.3340, grad_fn=&lt;UnbindBackward0&gt;), t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>223548</th>\n",
              "      <td>fffb5451268fb5ba</td>\n",
              "      <td>unicorn lair discovery supposedly a unicorn la...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>test</td>\n",
              "      <td>[unicorn, lair, discovery, supposedly, unicorn...</td>\n",
              "      <td>39</td>\n",
              "      <td>[196053, 104443, 49955, 181210, 196053, 104443...</td>\n",
              "      <td>[[tensor(0.5316, grad_fn=&lt;UnbindBackward0&gt;), t...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>63513 rows × 13 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-13fb864d-7190-4d5c-b7ad-d20c0a9562f2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-13fb864d-7190-4d5c-b7ad-d20c0a9562f2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-13fb864d-7190-4d5c-b7ad-d20c0a9562f2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function that calculates average vector\n",
        "\n",
        "def average_tensor(x):\n",
        "  tensor_d = torch.zeros((1,10))\n",
        "  for t in x:\n",
        "    tensor_d += t\n",
        "  return tensor_d/x.shape[0]"
      ],
      "metadata": {
        "id": "Uzp_ma5FOMhy"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create train tensor"
      ],
      "metadata": {
        "id": "thMk_TU8rWPJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training tensor (it's the average of all words of each comment)\n",
        "\n",
        "X_train_tensor = torch.zeros((df_train.shape[0],10))\n",
        "j = 0\n",
        "for i in df_train.index:\n",
        "  dtensor = df_train.loc[i,'vect_tensor']\n",
        "  X_train_tensor[j,:] = average_tensor(dtensor)\n",
        "  j+=1"
      ],
      "metadata": {
        "id": "TTtf8mwiUROQ"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tensors must be saved in order to continue with the execution because the training tensor creation process consumes a lot of RAM"
      ],
      "metadata": {
        "id": "5sLOpOq3EoQ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(X_train_tensor,'/content/drive/MyDrive/ML/train_tensor.pt') # guardar tensor"
      ],
      "metadata": {
        "id": "lHiHwqnrUd7O"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(Y_train,'/content/drive/MyDrive/ML/train_labels.pt')# guardar tensor"
      ],
      "metadata": {
        "id": "N60NPBByVNj-"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Session must be restarted and rerun everything except the training tensor creation"
      ],
      "metadata": {
        "id": "ZaD9W9HhEtrY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create test tensor"
      ],
      "metadata": {
        "id": "yx5OtpuDtRag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's create test tensor\n",
        "X_test_tensor = torch.zeros((df_test.shape[0],10))\n",
        "j = 0\n",
        "for i in df_test.index:\n",
        "  dtensor = df_test.loc[i,'vect_tensor']\n",
        "  X_test_tensor[j,:] = average_tensor(dtensor)\n",
        "  j += 1"
      ],
      "metadata": {
        "id": "iKyieVaZ7Doi"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save tensors just in case\n",
        "torch.save(X_test_tensor,'/content/drive/MyDrive/ML/test_tensor.pt')\n",
        "torch.save(Y_test,'/content/drive/MyDrive/ML/test_labels.pt')"
      ],
      "metadata": {
        "id": "K6F86loQ-bWS"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load all tensors"
      ],
      "metadata": {
        "id": "Ft60R_PytV_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_tensor=torch.load('/content/drive/MyDrive/ML/train_tensor.pt')\n",
        "Y_train_tensor=torch.load('/content/drive/MyDrive/ML/train_labels.pt')\n",
        "X_test_tensor=torch.load('/content/drive/MyDrive/ML/test_tensor.pt')\n",
        "Y_test_tensor=torch.load('/content/drive/MyDrive/ML/test_labels.pt')"
      ],
      "metadata": {
        "id": "eOo8jwmaWLfP"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ref = Y_train_tensor.sum(axis=1)"
      ],
      "metadata": {
        "id": "zSyiKxzEKzRg"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Get 20 thousand random samples of 0,0,0,0,0,0 (indices only)\n",
        "i =0\n",
        "chosen = []\n",
        "while len(chosen)<=20000:\n",
        "    n = np.random.choice([i for i in range(len(Y_train_tensor))])\n",
        "    if ref[n] == 0 and n not in chosen:\n",
        "        chosen.append(n)\n",
        "        #print(len(chosen))"
      ],
      "metadata": {
        "id": "7aJFe7gsLFW7"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.DataFrame({'n':chosen}).to_csv('/content/drive/MyDrive/ML/ceros.csv')"
      ],
      "metadata": {
        "id": "DgZDrBv4SNBw"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Indices of those that are not only 0,0,0,0,0,0 are saved\n",
        "chosen_2 = []\n",
        "for i in range(len(ref)):\n",
        "    if ref[i] >= 1:\n",
        "        chosen_2.append(i)"
      ],
      "metadata": {
        "id": "Nx7wEZ-6MWQe"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's join the indices\n",
        "final_chosen = chosen+chosen_2\n",
        "final_chosen.sort()"
      ],
      "metadata": {
        "id": "fFvrbwOZO_-d"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the balanced dataset\n",
        "X_train_tensor = X_train_tensor[final_chosen]\n",
        "Y_train_tensor = Y_train_tensor[final_chosen]"
      ],
      "metadata": {
        "id": "WYmSv7f-Ps5t"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train model"
      ],
      "metadata": {
        "id": "ARsB7oh-ty1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print('Running on device: {}'.format(device))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksZbGpaiYa8g",
        "outputId": "8fcccbf0-4246-4758-eda0-2c0fabef6d03"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on device: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "Rkldj-WQt3T9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Base model: network of a neuron with 6 outputs\n",
        "class base_line(nn.Module):\n",
        "  def __init__(self,fin,out):\n",
        "    super(base_line,self).__init__()\n",
        "    self.out = out\n",
        "    self.fin = fin\n",
        "    self.fc1 = nn.Linear(self.fin,2048)\n",
        "    self.fc2 = nn.Linear(2048,1024)\n",
        "    self.fc3 = nn.Linear(1024,512)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.fc4 = nn.Linear(512,self.out)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self,x):\n",
        "    out = self.fc1(x)\n",
        "    out = self.fc2(out)\n",
        "    out = self.fc3(out)\n",
        "    our = self.relu(out)\n",
        "    out = self.fc4(out)\n",
        "    out = self.sigmoid(out)\n",
        "    return out"
      ],
      "metadata": {
        "id": "VfhhFLPxxH6f"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math\n",
        "\n",
        "# Let's check that there are no NaNs in train\n",
        "for i in range(len(X_train_tensor)):\n",
        "  if math.isnan(X_train_tensor[i].mean()):\n",
        "    print(i)"
      ],
      "metadata": {
        "id": "cfBVYRO3bMmf"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check that there are no NaNs in test\n",
        "for i in range(len(X_test_tensor)):\n",
        "  if math.isnan(X_test_tensor[i].mean()):\n",
        "    print(i)"
      ],
      "metadata": {
        "id": "mReRmHr8_-Ln"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_tensor = X_train_tensor.detach()\n",
        "Y_train_tensor = Y_train_tensor.detach()"
      ],
      "metadata": {
        "id": "JmYwmhyRTAE9"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_tensor.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfmnQZgLAPAp",
        "outputId": "e85f744b-f066-4854-b6f4-865e18901ff9"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([36221, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create datasets\n",
        "X_train = TensorDataset(X_train_tensor,Y_train_tensor)\n",
        "X_test = TensorDataset(X_test_tensor,Y_test_tensor)"
      ],
      "metadata": {
        "id": "OszpTBKTYB_T"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataloaders\n",
        "train_dataloader = DataLoader(X_train,batch_size=32,shuffle=True)\n",
        "test_dataloader = DataLoader(X_test,batch_size=32,shuffle=True)"
      ],
      "metadata": {
        "id": "twYH47KvZzyP"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define optimizing model and times\n",
        "model = base_line(10,6).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "loss_fn = torch.nn.BCELoss()"
      ],
      "metadata": {
        "id": "8POgVwZ2W5DS"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "epochs = 30\n",
        "max_acc = 0.75\n",
        "\n",
        "for e in range(epochs):\n",
        "  loss_run_train = []\n",
        "  acc_run_train = []\n",
        "\n",
        "  print(f'Epoch {e}/{epochs}')\n",
        "  model = model.train()\n",
        "  for x,y in train_dataloader:\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "    z = model(x)\n",
        "    \n",
        "    loss = loss_fn(z,y.float())\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    loss_run_train.append(loss.item())\n",
        "\n",
        "    preds = []\n",
        "    for t in z:\n",
        "      preds.append([1 if i>0.5 else 0 for i in t])\n",
        "\n",
        "    corrects = 0\n",
        "    for p,yo in zip(y,torch.tensor(preds).to(device)):\n",
        "      if torch.sum(p==yo).item()==6:\n",
        "        #print(p,yo)\n",
        "        corrects += 1\n",
        "    #print(f'corrects:{corrects}/{y.shape[0]}')\n",
        "    \n",
        "    acc_run_train.append(corrects/y.shape[0])\n",
        "  \n",
        "  model.eval()\n",
        "  loss_run_test = []\n",
        "  acc_run_test = []\n",
        "  for x,y in test_dataloader:\n",
        "      x = x.to(device)\n",
        "      y = y.to(device)\n",
        "      z = model(x)\n",
        "      loss = loss_fn(z,y.float())\n",
        "      loss_run_test.append(loss.item())\n",
        "      preds = []\n",
        "\n",
        "      # las predicciones se cuentan correctas cuando todo el tensor de salida de 6 posiciones es identico al ground truth\n",
        "      ######\n",
        "      for t in z:\n",
        "          preds.append([1 if i>0.5 else 0 for i in t])\n",
        "          corrects=0\n",
        "      for p,yo in zip(y,torch.tensor(preds).to(device)):\n",
        "          if torch.sum(p==yo).item()==6:\n",
        "              #print(p,yo)\n",
        "              corrects += 1\n",
        "      ########\n",
        "      acc_run_test.append(corrects/y.shape[0])\n",
        "  if np.mean(acc_run_test)>max_acc:\n",
        "      print('Saving best model!')\n",
        "      max_acc = np.mean(acc_run_test)\n",
        "      torch.save(model.state_dict(), f'/content/drive/MyDrive/ML/model_{e}_{round(100*max_acc,2)}.pth')\n",
        "\n",
        "  print(f' + [train] loss:{np.mean(loss_run_train)} acc:{np.mean(acc_run_train)}')\n",
        "  print(f' - [test] loss:{np.mean(loss_run_test)} acc:{np.mean(acc_run_test)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVOk-jNVX9NI",
        "outputId": "8832ffe3-97a8-462f-8a27-e487478f3885"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/30\n",
            "Saving best model!\n",
            " + [train] loss:0.3547021203243269 acc:0.5027482256000975\n",
            " - [test] loss:0.23396374876763418 acc:0.812993073047859\n",
            "Epoch 1/30\n",
            "Saving best model!\n",
            " + [train] loss:0.34894296363680605 acc:0.5159381853905203\n",
            " - [test] loss:0.19348287638323733 acc:0.8577078085642317\n",
            "Epoch 2/30\n",
            " + [train] loss:0.34952715656624667 acc:0.5127082825636652\n",
            " - [test] loss:0.2083922236181927 acc:0.8260125944584383\n",
            "Epoch 3/30\n",
            "Saving best model!\n",
            " + [train] loss:0.34779263676107985 acc:0.5187901714999391\n",
            " - [test] loss:0.19161069544421636 acc:0.8635887909319899\n",
            "Epoch 4/30\n",
            " + [train] loss:0.3466790549641363 acc:0.5211033645059095\n",
            " - [test] loss:0.22428721631654264 acc:0.8159370277078085\n",
            "Epoch 5/30\n",
            " + [train] loss:0.3467622251634884 acc:0.5182599457779944\n",
            " - [test] loss:0.21372695878260684 acc:0.8120730478589422\n",
            "Epoch 6/30\n",
            " + [train] loss:0.3462338454152586 acc:0.5202894632630681\n",
            " - [test] loss:0.21625586884748424 acc:0.8380673803526448\n",
            "Epoch 7/30\n",
            " + [train] loss:0.3456705396751422 acc:0.5234584424881199\n",
            " - [test] loss:0.22370856236150344 acc:0.80883879093199\n",
            "Epoch 8/30\n",
            " + [train] loss:0.3464982566611295 acc:0.5187044976849031\n",
            " - [test] loss:0.214678909571405 acc:0.810375314861461\n",
            "Epoch 9/30\n",
            " + [train] loss:0.345009993603815 acc:0.5250291290971122\n",
            " - [test] loss:0.2007912427825051 acc:0.8414452141057934\n",
            "Epoch 10/30\n",
            " + [train] loss:0.3456452190086193 acc:0.5225855215060314\n",
            " - [test] loss:0.2160898378273702 acc:0.8263841309823677\n",
            "Epoch 11/30\n",
            " + [train] loss:0.34478667304709604 acc:0.5251176587059827\n",
            " - [test] loss:0.21451737084977274 acc:0.8430869017632242\n",
            "Epoch 12/30\n",
            " + [train] loss:0.34505850875704114 acc:0.5251119471183137\n",
            " - [test] loss:0.21268444129591926 acc:0.8367783375314862\n",
            "Epoch 13/30\n",
            " + [train] loss:0.3450845767720849 acc:0.5190081637626417\n",
            " - [test] loss:0.20357860628543634 acc:0.8451309823677582\n",
            "Epoch 14/30\n",
            " + [train] loss:0.3449542789782526 acc:0.5250234175094431\n",
            " - [test] loss:0.20961110285607634 acc:0.8338211586901764\n",
            "Epoch 15/30\n",
            " + [train] loss:0.344338934657957 acc:0.5237782913975875\n",
            " - [test] loss:0.20087089053629628 acc:0.8497462216624686\n",
            "Epoch 16/30\n",
            " + [train] loss:0.3451843221438226 acc:0.5228701489582064\n",
            " - [test] loss:0.2113010995634257 acc:0.8299414357682621\n",
            "Epoch 17/30\n",
            "Saving best model!\n",
            " + [train] loss:0.3442675633086122 acc:0.5244275085293043\n",
            " - [test] loss:0.18781464952665852 acc:0.8666158690176323\n",
            "Epoch 18/30\n",
            " + [train] loss:0.3442944970126708 acc:0.5249796286706471\n",
            " - [test] loss:0.2160008451275021 acc:0.8387890428211587\n",
            "Epoch 19/30\n",
            " + [train] loss:0.34468669940589175 acc:0.5237621085658585\n",
            " - [test] loss:0.1990070335247955 acc:0.8583602015113351\n",
            "Epoch 20/30\n",
            " + [train] loss:0.3448325924759619 acc:0.522072430547094\n",
            " - [test] loss:0.21484600920370905 acc:0.8264937027707809\n",
            "Epoch 21/30\n",
            " + [train] loss:0.34422818134298594 acc:0.5244608261240404\n",
            " - [test] loss:0.2125086667405568 acc:0.8417285894206549\n",
            "Epoch 22/30\n",
            " + [train] loss:0.3443317058966775 acc:0.5254841522480809\n",
            " - [test] loss:0.20585724523370752 acc:0.8282569269521411\n",
            "Epoch 23/30\n",
            " + [train] loss:0.3447556982586957 acc:0.5226550124893384\n",
            " - [test] loss:0.20854668152572525 acc:0.84572040302267\n",
            "Epoch 24/30\n",
            " + [train] loss:0.34405701606149386 acc:0.5252033325210187\n",
            " - [test] loss:0.20453982687642053 acc:0.8401316120906801\n",
            "Epoch 25/30\n",
            " + [train] loss:0.3444873709165797 acc:0.524673106799074\n",
            " - [test] loss:0.20037651778543625 acc:0.8499036523929472\n",
            "Epoch 26/30\n",
            "Saving best model!\n",
            " + [train] loss:0.3440696758484672 acc:0.5250700621420739\n",
            " - [test] loss:0.1970959850274946 acc:0.8711612090680101\n",
            "Epoch 27/30\n",
            " + [train] loss:0.3441837038779006 acc:0.5252090441086876\n",
            " - [test] loss:0.2054599339250353 acc:0.8470270780856424\n",
            "Epoch 28/30\n",
            " + [train] loss:0.3440604363913671 acc:0.5246759625929085\n",
            " - [test] loss:0.20674630109248893 acc:0.8449175062972293\n",
            "Epoch 29/30\n",
            " + [train] loss:0.3444217341185248 acc:0.5234470193127818\n",
            " - [test] loss:0.21564486250469006 acc:0.8454055415617129\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Eval model"
      ],
      "metadata": {
        "id": "b1Wov4q9Z-Xf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def converter(words):\n",
        "    return average_tensor( embedding(torch.tensor(coded_words(words.split(), word_dict))))"
      ],
      "metadata": {
        "id": "zsWoKNMPGwnn"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"If ya not... still fu*k u\"\n",
        "text = clean_text(text)\n",
        "out = converter(text)\n",
        "res = model(out.to(device))\n",
        "res"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6VwqhFFG0UU",
        "outputId": "c3b71bf4-a4ea-4db3-e267-1ac07640de76"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.4134, 0.0573, 0.2934, 0.0075, 0.2001, 0.0347]], device='cuda:0',\n",
              "       grad_fn=<SigmoidBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res[0][0].item()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnpUAbaivPN8",
        "outputId": "7e9b3db0-72e0-48ab-de39-96357be48b92"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.41337257623672485"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res[0][1].item()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0lcQtWZ0A-_",
        "outputId": "36b22a8d-dba4-4969-93d5-f0caaa84f679"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.057300958782434464"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(\"/content/drive/MyDrive/ML/word_dict.json\", \"w\") as write_file:\n",
        "    json.dump(word_dict, write_file, indent=4)"
      ],
      "metadata": {
        "id": "iFzI0Wnm0IuV"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "An-K2Z7j0Tzo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}